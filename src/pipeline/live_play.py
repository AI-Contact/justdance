"""
ì‹¤ì‹œê°„ ì›¹ìº  ë¹„êµ ëª¨ë“œ
"""

import os
import time
import math
import cv2
import numpy as np

from .extractor import PoseExtractor
from .matcher import OnlineMatcher, read_frame_at
from .pose_utils import (
    BONES, ANGLE_TRIPLES,
    normalize_landmarks, pose_embedding, angle_feedback,
    draw_colored_skeleton
)
from .similarity import (
    build_static_feature_weights, weighted_cosine, exp_moving_avg, load_weights_for_video
)
from .rest_intervals import load_rest_intervals_json, in_intervals
from .utils import parse_bgr, play_beep, start_ref_audio_player, stop_ref_audio_player


def live_play(ref_path, ref_lm_path, camera=0, search_radius=0, ema_alpha=0.9, show_feedback=True,
              ref_video=None, ref_stride=2, loop_ref=False, rtf=1.0, model_complexity=1,
              warmup_sec=5.0, countdown_color="0,215,255", countdown_beep=True, play_ref_audio=True,
              w_pose=1.0, w_motion=0.0, time_penalty=0.0, late_grace=5, late_penalty=0.05,
              rest_json: str | None = None,
              out_video: str | None = None,
              overlay_alpha: float = 0.5):
    """
    compare_videos()ì™€ ë™ì¼í•œ ë¡œì§/ì—°ì¶œì„ ìœ ì§€í•˜ê³ ,
    ì…ë ¥ë§Œ ì‹¤ì‹œê°„ ì¹´ë©”ë¼ë¡œ ë°”ê¾¼ ë²„ì „.
    """
    rest_json = "data/rest_exc.json"
    ref = np.load(ref_path)
    #lm_path = ref_path.replace('.npy', '_lm.npy')
    #try:
    #    ref_lm = np.load(lm_path, allow_pickle=True)
    #except Exception:
    #    ref_lm = None
    ref_lm = np.load(ref_lm_path)
    camera = 0
    search_radius = 0
    ema_alpha = 0.95
    show_feedback = True
    ref_stride = 2
    rtf = 1.0
    model_complexity = 1
    warmup_sec = 5.0
    countdown_beep = True
    play_ref_audio = True
    w_pose = 1.0
    w_motion = 0.0
    time_penalty = 0.0
    late_grace = 5
    late_penalty = 0.05

    # --- Reference video ì¤€ë¹„ (ë™ì¼) ---
    ref_cap = None
    ref_fps = 30.0
    ref_total_frames = None
    if ref_video is not None and os.path.exists(ref_video):
        ref_cap = cv2.VideoCapture(ref_video)
        if ref_cap.isOpened():
            fps_val = ref_cap.get(cv2.CAP_PROP_FPS)
            if fps_val and fps_val > 1e-3:
                ref_fps = float(fps_val)
            ref_total_frames = int(ref_cap.get(cv2.CAP_PROP_FRAME_COUNT))
        else:
            ref_cap = None

    # ì„ë² ë”© í•œ ìŠ¤í…ì˜ ì‹œê°„ ê°„ê²©(ë™ì¼)
    step_sec = ref_stride / (ref_fps * max(1e-6, rtf))

    # --- ì‰¬ëŠ” êµ¬ê°„ ë¡œë“œ(ë™ì¼) ---
    rest_intervals_emb = []
    if rest_json and ref_video is not None:
        ref_base = os.path.basename(ref_video)
        rest_intervals_emb = load_rest_intervals_json(rest_json, ref_base, ref_fps, ref_stride)

    # --- ì¹´ë©”ë¼ ---
    cap = cv2.VideoCapture(camera)
    if not cap.isOpened():
        raise SystemExit(f"ì¹´ë©”ë¼ ì—´ê¸° ì‹¤íŒ¨: {camera}")

    # --- out_video ì¤€ë¹„ (compare_videosì™€ ë™ì¼í•œ í•©ì„±í­ìœ¼ë¡œ) ---
    writer = None
    out_w = out_h = None
    if out_video:
        # ì¹´ë©”ë¼ í•œ í”„ë ˆì„ ì½ì–´ í¬ê¸° íŒŒì•…
        ok_probe, probe = cap.read()
        if not ok_probe:
            cap.release()
            if ref_cap is not None:
                ref_cap.release()
            raise SystemExit("ì¹´ë©”ë¼ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        # ìš°ì¸¡ ref ì²« í”„ë ˆì„
        display_right0 = None
        if ref_cap is not None:
            ref_cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
            ok_r, rframe0 = ref_cap.read()
            if ok_r:
                display_right0 = rframe0
        h, w = probe.shape[:2]
        if display_right0 is not None:
            rh, rw = display_right0.shape[:2]
            if rh != h:
                scale = h / float(rh)
                display_right0 = cv2.resize(display_right0, (int(rw*scale), h), interpolation=cv2.INTER_LINEAR)
            out_w = w + display_right0.shape[1]
            out_h = h
        else:
            out_w, out_h = w, h
        # í”„ë ˆì„ë ˆì´íŠ¸ëŠ” ref_fps ë˜ëŠ” 30ìœ¼ë¡œ ì €ì¥
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        writer = cv2.VideoWriter(out_video, fourcc, ref_fps if ref_fps > 1e-3 else 30.0, (out_w, out_h))
        # ref_cap í¬ì§€ì…˜ ë³µêµ¬
        if ref_cap is not None:
            ref_cap.set(cv2.CAP_PROP_POS_FRAMES, 0)

    pe = PoseExtractor(static_image_mode=False, model_complexity=model_complexity)
    matcher = OnlineMatcher(ref)

    prev_live_emb = None
    prev_ref_emb  = None
    score_ema = 0.0
    score_window = []  # â† compare_videosì™€ ë™ì¼: ë¡¤ë§ í‰ê·  ë²„í¼
    last_feedback = []
    font = cv2.FONT_HERSHEY_SIMPLEX

    cd_color = parse_bgr(countdown_color, default=(0,215,255))
    last_beep_whole = None
    ref_audio_proc = None

    start_total = time.perf_counter()
    sync_start_t = None
    warmup_done = False
    ref_frame_cur = 0
    grade_counts = {"PERFECT": 0, "GOOD": 0, "BAD": 0}
    graded_total = 0
    last_grade_time = 0.0  # ë§ˆì§€ë§‰ìœ¼ë¡œ ë“±ê¸‰ì„ ë§¤ê¸´ ì‹œê°„
    grade_interval = 3.0   # ë“±ê¸‰ ë§¤ê¸°ëŠ” ê°„ê²© (ì´ˆ)
    grade_history = []     # 3ì´ˆê°„ ë“±ê¸‰ íˆìŠ¤í† ë¦¬ [(timestamp, grade), ...]
    window_name = "Fitness Dance - Live (left: YOU, right: REFERENCE)"

    while True:
        now = time.perf_counter()
        elapsed_total = now - start_total

        ok, frame = cap.read()
        if not ok:
            break
        display_left = frame.copy()

        # ----- Warmup (ê·¸ëŒ€ë¡œ ìœ ì§€) -----
        static_sim = 0.0
        motion_sim = 0.0
        motion_mag_match = 1.0

        if not warmup_done and elapsed_total < warmup_sec:
            display_right = None
            if ref_cap is not None:
                ref_cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                ok_ref, ref_frame = ref_cap.read()
                if ok_ref:
                    display_right = ref_frame
            h, w = display_left.shape[:2]
            if display_right is not None:
                rh, rw = display_right.shape[:2]
                if rh != h:
                    scale = h / float(rh)
                    display_right = cv2.resize(display_right, (int(rw*scale), h), interpolation=cv2.INTER_LINEAR)
                combined = np.hstack([display_left, display_right])
            else:
                combined = display_left
            remain = max(0.0, warmup_sec - elapsed_total)
            cv2.putText(combined, f"ì¤€ë¹„í•˜ì„¸ìš”... {remain:0.1f}s", (20, 40), font, 1.0, cd_color, 2, cv2.LINE_AA)

            if countdown_beep:
                whole = int(math.ceil(remain))
                if whole != last_beep_whole and 1 <= whole <= 3:
                    play_beep(freq=700.0 + 100.0*whole, dur=0.15)
                    last_beep_whole = whole
                if remain <= 0.05 and last_beep_whole != 0:
                    play_beep(freq=1400.0, dur=0.25); last_beep_whole = 0

            if ref_cap is not None:
                cv2.putText(combined, "REFERENCE (5ì´ˆ í›„ ì‹œì‘)", (w + 20, 40), font, 1.0, (255,255,255), 2, cv2.LINE_AA)
            cv2.imshow(window_name, combined)
            if (cv2.waitKey(1) & 0xFF) in (27, ord('q')): break
            # out_videoì—ëŠ” ì›Œë°ì—… í™”ë©´ ì €ì¥ ì•ˆ í•¨(ì›í•˜ë©´ ì—¬ê¸°ì—ì„œ writer.write(combined))
            continue

        if not warmup_done and elapsed_total >= warmup_sec:
            warmup_done = True
            sync_start_t = time.perf_counter()
            score_ema = 0.0
            score_window.clear()
            last_feedback = ["Start! Follow the reference video ğŸ’ª"]
            if ref_cap is not None:
                ref_cap.set(cv2.CAP_PROP_POS_FRAMES, 0); ref_frame_cur = 0
            if play_ref_audio and ref_video is not None and os.path.exists(ref_video):
                stop_ref_audio_player(ref_audio_proc)
                ref_audio_proc = start_ref_audio_player(ref_video_path=ref_video, start_sec=0.0)
            continue

        # ---- ë™ê¸° ì¸ë±ìŠ¤ ----
        elapsed = time.perf_counter() - sync_start_t if sync_start_t is not None else 0.0
        hint_idx = int(elapsed / step_sec)
        if ref_lm is not None: hint_idx = min(hint_idx, len(ref_lm) - 1)
        hint_idx = min(hint_idx, ref.shape[0] - 1)

        # ---- ì‰¬ëŠ” êµ¬ê°„ ì²˜ë¦¬(ë™ì¼) ----
        if in_intervals(hint_idx, rest_intervals_emb):
            display_right = None
            if ref_cap is not None:
                target_ref_frame = hint_idx * ref_stride
                ok_ref, ref_frame, ref_frame_cur = read_frame_at(ref_cap, target_ref_frame, ref_frame_cur)
                if ok_ref: display_right = ref_frame
            h, w = display_left.shape[:2]
            if display_right is not None:
                rh, rw = display_right.shape[:2]
                if rh != h:
                    scale = h / float(rh)
                    display_right = cv2.resize(display_right, (int(rw * scale), h), interpolation=cv2.INTER_LINEAR)
                combined = np.hstack([display_left, display_right])
            else:
                combined = display_left
            cv2.putText(combined, "REST", (20, 40), font, 1.4, (255,200,200), 3, cv2.LINE_AA)
            if ref_cap is not None:
                cv2.putText(combined, "REFERENCE", (w + 20, 40), font, 1.0, (255,255,255), 2, cv2.LINE_AA)
            if writer is not None: writer.write(combined)
            cv2.imshow(window_name, combined)
            if (cv2.waitKey(1) & 0xFF) in (27, ord('q')): break
            continue

        # ---- í¬ì¦ˆ ì¶”ë¡  ----
        arr = pe.infer(frame)
        if arr is not None:
            # (1) ìŠ¤ì¼ˆë ˆí†¤ ì˜¤ë²„ë ˆì´( compare_videosì™€ ë™ì¼ )
            overlay = display_left.copy()
            draw_colored_skeleton(overlay, arr)
            display_left = cv2.addWeighted(overlay, overlay_alpha, display_left, 1 - overlay_alpha, 0)

            # (2) ì„ë² ë”© ë° ë§¤ì¹­
            lm = normalize_landmarks(arr.copy())
            emb = pose_embedding(lm)
            sim, ref_idx = matcher.step_with_hint(emb, hint_idx=hint_idx, search_radius=search_radius)

            # (3) ê°€ì¤‘ì¹˜, ì •ì /ëª¨ì…˜ ìœ ì‚¬ë„
            if ref_lm is not None and ref_idx < len(ref_lm):
                #w_feat = build_static_feature_weights(BONES, ANGLE_TRIPLES, lm_for_vis=ref_lm[ref_idx])
                region_w, angle_w = load_weights_for_video(ref_video)
                w_feat, region_w, angle_w = build_static_feature_weights(
                    BONES, ANGLE_TRIPLES,
                    lm_for_vis=ref_lm[ref_idx],
                    region_w=region_w,
                    angle_w=angle_w
                )
                #print(w_feat)
            else:
                w_feat = build_static_feature_weights(BONES, ANGLE_TRIPLES, lm_for_vis=None)

            static_sim = weighted_cosine(matcher.ref[ref_idx], emb, w_feat)
            motion_sim = 0.0; motion_mag_match = 1.0
            ref_emb = matcher.ref[ref_idx]
            if prev_live_emb is not None and prev_ref_emb is not None:
                d_live = emb - prev_live_emb
                d_ref  = ref_emb - prev_ref_emb
                motion_sim = weighted_cosine(d_live, d_ref, w_feat)
                n_live = np.linalg.norm(d_live); n_ref = np.linalg.norm(d_ref)
                if n_live > 1e-6 or n_ref > 1e-6:
                    motion_mag_match = min(n_live, n_ref) / (max(n_live, n_ref) + 1e-8)

            # (4) ì‹œê°„ ì´íƒˆ íŒ¨ë„í‹°(ë™ì¼)
            if search_radius > 0:
                delta = ref_idx - hint_idx
                if delta < 0:
                    late_dt = -delta
                    if late_dt <= late_grace: align_factor = 1.0
                    else:
                        span = max(1, search_radius - late_grace)
                        align_factor = 1.0 - late_penalty * ((late_dt - late_grace) / span)
                elif delta > 0:
                    span = max(1, search_radius)
                    align_factor = 1.0 - time_penalty * (delta / span)
                else:
                    align_factor = 1.0
                align_factor = max(0.0, min(1.0, align_factor))
            else:
                align_factor = 1.0

            blended = (w_pose * static_sim) + (w_motion * motion_sim * motion_mag_match)
            score = ((blended + 1.0) * 0.5) * align_factor

            score_ema = exp_moving_avg(score_ema, score, alpha=ema_alpha)

            prev_live_emb = emb
            prev_ref_emb  = ref_emb

            if show_feedback and ref_lm is not None and ref_idx < len(ref_lm):
                msgs = angle_feedback(lm, ref_lm[ref_idx], angle_tol_deg=10.0)
                last_feedback = msgs if len(msgs)>0 else ["Good! Nice pose ğŸ‘"]
        else:
            last_feedback = ["cannot extract pose"]

        # ---- ìš°ì¸¡ ref í”„ë ˆì„ ì¤€ë¹„ (ë™ì¼) ----
        display_right = None
        if ref_cap is not None:
            target_ref_frame = hint_idx * ref_stride
            ok_ref, ref_frame, ref_frame_cur = read_frame_at(ref_cap, target_ref_frame, ref_frame_cur)
            if not ok_ref and loop_ref and ref_total_frames:
                ref_cap.set(cv2.CAP_PROP_POS_FRAMES, 0); ref_frame_cur = 0
                ok_ref, ref_frame, ref_frame_cur = read_frame_at(ref_cap, 0, ref_frame_cur)
            if ok_ref:
                display_right = ref_frame

        # ---- í•©ì„± & ì˜¤ë²„ë ˆì´( compare_videosì™€ ì™„ì „ ë™ì¼ ) ----
        h, w = display_left.shape[:2]
        if display_right is not None:
            rh, rw = display_right.shape[:2]
            if rh != h:
                scale = h / float(rh)
                display_right = cv2.resize(display_right, (int(rw*scale), h), interpolation=cv2.INTER_LINEAR)
            combined = np.hstack([display_left, display_right])
        else:
            combined = display_left

        # ë¡¤ë§ í‰ê· (3ì´ˆ) + 50~95 -> 0~100 ì •ê·œí™” & ë“±ê¸‰
        score_window.append(score_ema)
        # ì¹´ë©”ë¼ fpsë¥¼ ëª¨ë¥¼ ìˆ˜ ìˆì–´ ref_fps ê¸°ì¤€ 3ì´ˆ ìœˆë„ìš°ë¡œ ì‚¬ìš©
        if len(score_window) > int(ref_fps * 3):
            score_window.pop(0)
        avg_score = float(np.nan_to_num(np.mean(score_window) if len(score_window)>0 else score_ema, nan=0.0, posinf=1.0, neginf=0.0))

        avg_50_95 = avg_score * 100.0
        avg_pct   = ((avg_50_95 - 50.0) / 45.0) * 100.0
        # ë³´ê¸° ì¢‹ê²Œ 0~100ë¡œ í´ë¦½
        avg_pct   = float(np.clip(avg_pct, 0.0, 100.0))

        grade_text, grade_color = "", (255,255,255)
        if avg_pct >= 70:
            grade_text, grade_color = "PERFECT", (0,255,0)
        elif avg_pct >= 55:
            grade_text, grade_color = "GOOD", (0,200,255)
        else:
            grade_text, grade_color = "BAD", (255,0,0)

        # ë§¤ í”„ë ˆì„ë§ˆë‹¤ ë“±ê¸‰ì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        current_time = time.perf_counter()
        if grade_text and sync_start_t is not None:
            grade_history.append((current_time, grade_text))

            # 3ì´ˆë³´ë‹¤ ì˜¤ë˜ëœ íˆìŠ¤í† ë¦¬ ì œê±°
            cutoff_time = current_time - grade_interval
            grade_history[:] = [(t, g) for t, g in grade_history if t > cutoff_time]

            # 3ì´ˆë§ˆë‹¤ í•œ ë²ˆì”©ë§Œ ë“±ê¸‰ í‰ê°€ (warmup ì™„ë£Œ í›„ ê²½ê³¼ ì‹œê°„ ê¸°ì¤€)
            elapsed_since_start = current_time - sync_start_t
            if elapsed_since_start - last_grade_time >= grade_interval:
                # 3ì´ˆ ë™ì•ˆì˜ íˆìŠ¤í† ë¦¬ì—ì„œ ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ë“±ê¸‰ ê³„ì‚°
                if grade_history:
                    from collections import Counter
                    grade_counts_in_window = Counter(g for t, g in grade_history)
                    # ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ë“±ê¸‰ ì„ íƒ (ë™ì ì´ë©´ PERFECT > GOOD > BAD ìš°ì„ ìˆœìœ„)
                    most_common_grade = None
                    max_count = 0
                    for g in ['PERFECT', 'GOOD', 'BAD']:
                        if grade_counts_in_window[g] > max_count:
                            max_count = grade_counts_in_window[g]
                            most_common_grade = g

                    if most_common_grade and most_common_grade in grade_counts:
                        grade_counts[most_common_grade] += 1
                        graded_total += 1
                        last_grade_time = elapsed_since_start
                        # íˆìŠ¤í† ë¦¬ í´ë¦¬ì–´ (ìƒˆë¡œìš´ 3ì´ˆ ì‹œì‘)
                        grade_history.clear()


        cv2.putText(combined, f"Similarity(avg): {avg_pct:.1f}%  {avg_score*100:.1f}%", (20, 40), font, 1.0, (0,255,0), 2, cv2.LINE_AA)
        cv2.putText(combined, grade_text, (20, 90), font, 1.4, grade_color, 3, cv2.LINE_AA)
        y0 = 120
        for i, m in enumerate(last_feedback[:3]):
            cv2.putText(combined, m, (20, y0 + i*30), font, 1.0, (0,200,255), 2, cv2.LINE_AA)
        cv2.putText(combined, f"pose={static_sim:+.2f}  motion={motion_sim:+.2f}  mag={motion_mag_match:.2f}",
                    (20, y0 + 3*30), font, 0.6, (200,200,200), 1, cv2.LINE_AA)
        if ref_cap is not None:
            cv2.putText(combined, "REFERENCE", (w + 20, 40), font, 1.0, (255,255,255), 2, cv2.LINE_AA)

        if writer is not None:
            if out_w is None or out_h is None:
                out_h, out_w = combined.shape[:2]
            writer.write(combined)

        cv2.imshow(window_name, combined)
        if (cv2.waitKey(1) & 0xFF) in (27, ord('q')): break

        # ì„ë² ë”© ë ì²˜ë¦¬(ë™ì¼)
        if hint_idx >= ref.shape[0] - 1:
            if loop_ref:
                sync_start_t = time.perf_counter()
                if ref_cap is not None:
                    ref_cap.set(cv2.CAP_PROP_POS_FRAMES, 0); ref_frame_cur = 0
                if play_ref_audio and ref_video is not None and os.path.exists(ref_video):
                    stop_ref_audio_player(ref_audio_proc)
                    ref_audio_proc = start_ref_audio_player(ref_video_path=ref_video, start_sec=0.0)
            else:
                break

    # --- Final summary & rank ---
    if graded_total > 0:
        p_cnt = grade_counts["PERFECT"]
        g_cnt = grade_counts["GOOD"]
        b_cnt = grade_counts["BAD"]
        p_ratio = p_cnt / graded_total
        pg_ratio = (p_cnt + g_cnt) / graded_total

        # ë­í¬ ê·œì¹™(ì›í•˜ë©´ ìˆ«ìë§Œ ë°”ê¾¸ë©´ ë¨):
        # S: PERFECT >= 70%
        # A: PERFECT >= 50% ë˜ëŠ” (PERFECT+GOOD) >= 85%
        # B: (PERFECT+GOOD) >= 70%
        # C: (PERFECT+GOOD) >= 50%
        # F: ê·¸ ì™¸
        if p_ratio >= 0.70:
            final_rank = "S"
        elif p_ratio >= 0.50 or pg_ratio >= 0.85:
            final_rank = "A"
        elif pg_ratio >= 0.70:
            final_rank = "B"
        elif pg_ratio >= 0.50:
            final_rank = "C"
        else:
            final_rank = "F"

        # ì½˜ì†” ì¶œë ¥
        print("\n===== LIVE PLAY SUMMARY =====")
        print(f"Frames graded: {graded_total}")
        print(f"PERFECT: {p_cnt} ({p_ratio*100:.1f}%)")
        print(f"GOOD   : {g_cnt} ({(g_cnt/graded_total)*100:.1f}%)")
        print(f"BAD    : {b_cnt} ({(b_cnt/graded_total)*100:.1f}%)")
        print(f"FINAL RANK: {final_rank}")

        # í™”ë©´ì— 2ì´ˆê°„ ì˜¤ë²„ë ˆì´(ê°€ëŠ¥í•˜ë©´)
        try:
            if 'combined' in locals():
                canvas = combined.copy()
            else:
                canvas = np.zeros((480, 640, 3), dtype=np.uint8)
            y = 200
            cv2.putText(canvas, f"FINAL RANK: {final_rank}", (20, y), font, 1.4, (0,255,0), 3, cv2.LINE_AA); y += 50
            cv2.putText(canvas, f"PERFECT: {p_cnt} ({p_ratio*100:.1f}%)", (20, y), font, 1.0, (0,255,0), 2, cv2.LINE_AA); y += 35
            cv2.putText(canvas, f"GOOD   : {g_cnt} ({(g_cnt/graded_total)*100:.1f}%)", (20, y), font, 1.0, (0,200,255), 2, cv2.LINE_AA); y += 35
            cv2.putText(canvas, f"BAD    : {b_cnt} ({(b_cnt/graded_total)*100:.1f}%)", (20, y), font, 1.0, (0,0,255), 2, cv2.LINE_AA)

            cv2.imshow(window_name, canvas)
            if writer is not None:
                writer.write(canvas)
            cv2.waitKey(2000)
        except Exception:
            pass
    cap.release()
    if ref_cap is not None:
        ref_cap.release()
    stop_ref_audio_player(ref_audio_proc)
    if writer is not None:
        writer.release()
    cv2.destroyAllWindows()

